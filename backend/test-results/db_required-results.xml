<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="65" failures="19" skipped="0" tests="85" time="2.574" timestamp="2025-04-11T04:36:20.378213" hostname="codespaces-703d42"><testcase classname="app.tests.integration.test_digital_twin_integration_int" name="test_digital_twin_complete_workflow" time="0.035"><failure message="TypeError: Patient.__init__() got an unexpected keyword argument 'first_name'">@pytest.mark.asyncio
    async def test_digital_twin_complete_workflow():
        """
        Test a complete Digital Twin workflow from patient creation to treatment recommendations.
        This test demonstrates how all components work together in a typical scenario.
        """
        # Create a complete system using our factory
        system = MockDigitalTwinFactory.create_complete_system()
    
        # Extract components
        patient_repo = system["repositories"]["patient_repository"]
        digital_twin_repo = system["repositories"]["digital_twin_repository"]
        digital_twin_core = system["services"]["digital_twin_core"]
    
        # 1. Create a test patient
        patient_id = uuid.uuid4()
&gt;       patient = Patient(
            id=patient_id,
            first_name="Jane",
            last_name="Doe",
            date_of_birth=datetime.now() - timedelta(days=365 * 35),  # 35 years old
            gender="female", # Use string literal
            contact_info={
                "email": "jane.doe@example.com",
                "phone": "+1-555-123-4567"
            },
            # Use strings for diagnoses as per patient.py entity
            diagnoses=[
                "F32.1: Major depressive disorder, single episode, moderate",
                "F41.1: Generalized anxiety disorder"
            ],
            # Use strings for medications as per patient.py entity
            medications=[
                "Sertraline 50mg daily"
            ],
            allergies=["Penicillin"]
        )
E       TypeError: Patient.__init__() got an unexpected keyword argument 'first_name'

app/tests/integration/test_digital_twin_integration_int.py:31: TypeError</failure></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_process_text" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 282&#10;  def test_process_text(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:282&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 282
  def test_process_text(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:282</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_process_text_with_missing_prompt" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 307&#10;  def test_process_text_with_missing_prompt(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:307&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 307
  def test_process_text_with_missing_prompt(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:307</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_process_text_with_nonexistent_model" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 323&#10;  def test_process_text_with_nonexistent_model(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:323&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 323
  def test_process_text_with_nonexistent_model(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:323</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_depression_detection" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 341&#10;  def test_depression_detection(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:341&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 341
  def test_depression_detection(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:341</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_risk_assessment" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 370&#10;  def test_risk_assessment(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:370&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 370
  def test_risk_assessment(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:370</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_sentiment_analysis" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 399&#10;  def test_sentiment_analysis(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:399&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 399
  def test_sentiment_analysis(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:399</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_wellness_dimensions" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 427&#10;  def test_wellness_dimensions(client: TestClient, mock_auth, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:427&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 427
  def test_wellness_dimensions(client: TestClient, mock_auth, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:427</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_health_check" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 456&#10;  def test_health_check(client: TestClient, mock_services):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:456&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 456
  def test_health_check(client: TestClient, mock_services):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:456</error></testcase><testcase classname="app.tests.integration.test_mentallama_api" name="test_service_unavailable" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 470&#10;  def test_service_unavailable(client: TestClient, mock_auth):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:470&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py, line 470
  def test_service_unavailable(client: TestClient, mock_auth):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_auth, mock_services, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_mentallama_api.py:470</error></testcase><testcase classname="app.tests.integration.test_patient_repository_int.TestPatientRepository" name="test_create_patient" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 46&#10;      async def test_create_patient(self, repository, sample_patient_data):&#10;          &quot;&quot;&quot;Test creating a patient in the database.&quot;&quot;&quot;&#10;          # Create a patient&#10;          patient = await repository.create(sample_patient_data)&#10;&#10;          # Verify patient was created&#10;          assert patient is not None&#10;          assert patient.id == sample_patient_data[&quot;id&quot;]&#10;          assert patient.name == sample_patient_data[&quot;name&quot;]&#10;          assert patient.email == sample_patient_data[&quot;email&quot;]&#10;&#10;          # Clean up - delete the patient&#10;          await repository.delete(patient.id)&#10;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24&#10;      @pytest.fixture&#10;      async def repository(self, db_session):&#10;          &quot;&quot;&quot;Create a patient repository with a real DB session.&quot;&quot;&quot;&#10;          return PatientRepository(db_session)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 46
      async def test_create_patient(self, repository, sample_patient_data):
          """Test creating a patient in the database."""
          # Create a patient
          patient = await repository.create(sample_patient_data)

          # Verify patient was created
          assert patient is not None
          assert patient.id == sample_patient_data["id"]
          assert patient.name == sample_patient_data["name"]
          assert patient.email == sample_patient_data["email"]

          # Clean up - delete the patient
          await repository.delete(patient.id)
file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24
      @pytest.fixture
      async def repository(self, db_session):
          """Create a patient repository with a real DB session."""
          return PatientRepository(db_session)
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24</error></testcase><testcase classname="app.tests.integration.test_patient_repository_int.TestPatientRepository" name="test_get_patient_by_id" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 60&#10;      async def test_get_patient_by_id(self, repository, sample_patient_data):&#10;          &quot;&quot;&quot;Test retrieving a patient by ID from the database.&quot;&quot;&quot;&#10;          # Create a patient first&#10;          created_patient = await repository.create(sample_patient_data)&#10;&#10;          # Get the patient by ID&#10;          retrieved_patient = await repository.get_by_id(created_patient.id)&#10;&#10;          # Verify the patient was retrieved correctly&#10;          assert retrieved_patient is not None&#10;          assert retrieved_patient.id == created_patient.id&#10;          assert retrieved_patient.name == created_patient.name&#10;          assert retrieved_patient.email == created_patient.email&#10;&#10;          # Clean up&#10;          await repository.delete(created_patient.id)&#10;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24&#10;      @pytest.fixture&#10;      async def repository(self, db_session):&#10;          &quot;&quot;&quot;Create a patient repository with a real DB session.&quot;&quot;&quot;&#10;          return PatientRepository(db_session)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 60
      async def test_get_patient_by_id(self, repository, sample_patient_data):
          """Test retrieving a patient by ID from the database."""
          # Create a patient first
          created_patient = await repository.create(sample_patient_data)

          # Get the patient by ID
          retrieved_patient = await repository.get_by_id(created_patient.id)

          # Verify the patient was retrieved correctly
          assert retrieved_patient is not None
          assert retrieved_patient.id == created_patient.id
          assert retrieved_patient.name == created_patient.name
          assert retrieved_patient.email == created_patient.email

          # Clean up
          await repository.delete(created_patient.id)
file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24
      @pytest.fixture
      async def repository(self, db_session):
          """Create a patient repository with a real DB session."""
          return PatientRepository(db_session)
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24</error></testcase><testcase classname="app.tests.integration.test_patient_repository_int.TestPatientRepository" name="test_update_patient" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 77&#10;      async def test_update_patient(self, repository, sample_patient_data):&#10;          &quot;&quot;&quot;Test updating a patient in the database.&quot;&quot;&quot;&#10;          # Create a patient first&#10;          created_patient = await repository.create(sample_patient_data)&#10;&#10;          # Update data&#10;          update_data = {&#10;              &quot;name&quot;: &quot;Updated Name&quot;,&#10;              &quot;email&quot;: &quot;updated.email@example.com&quot;&#10;          }&#10;&#10;          # Update the patient&#10;          updated_patient = await repository.update(created_patient.id, update_data)&#10;&#10;          # Verify the update&#10;          assert updated_patient is not None&#10;          assert updated_patient.id == created_patient.id&#10;          assert updated_patient.name == update_data[&quot;name&quot;]&#10;          assert updated_patient.email == update_data[&quot;email&quot;]&#10;&#10;          # Double-check by retrieving again&#10;          retrieved_patient = await repository.get_by_id(created_patient.id)&#10;          assert retrieved_patient.name == update_data[&quot;name&quot;]&#10;&#10;          # Clean up&#10;          await repository.delete(created_patient.id)&#10;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24&#10;      @pytest.fixture&#10;      async def repository(self, db_session):&#10;          &quot;&quot;&quot;Create a patient repository with a real DB session.&quot;&quot;&quot;&#10;          return PatientRepository(db_session)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 77
      async def test_update_patient(self, repository, sample_patient_data):
          """Test updating a patient in the database."""
          # Create a patient first
          created_patient = await repository.create(sample_patient_data)

          # Update data
          update_data = {
              "name": "Updated Name",
              "email": "updated.email@example.com"
          }

          # Update the patient
          updated_patient = await repository.update(created_patient.id, update_data)

          # Verify the update
          assert updated_patient is not None
          assert updated_patient.id == created_patient.id
          assert updated_patient.name == update_data["name"]
          assert updated_patient.email == update_data["email"]

          # Double-check by retrieving again
          retrieved_patient = await repository.get_by_id(created_patient.id)
          assert retrieved_patient.name == update_data["name"]

          # Clean up
          await repository.delete(created_patient.id)
file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24
      @pytest.fixture
      async def repository(self, db_session):
          """Create a patient repository with a real DB session."""
          return PatientRepository(db_session)
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24</error></testcase><testcase classname="app.tests.integration.test_patient_repository_int.TestPatientRepository" name="test_delete_patient" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 104&#10;      async def test_delete_patient(self, repository, sample_patient_data):&#10;          &quot;&quot;&quot;Test deleting a patient from the database.&quot;&quot;&quot;&#10;          # Create a patient first&#10;          created_patient = await repository.create(sample_patient_data)&#10;&#10;          # Delete the patient&#10;          result = await repository.delete(created_patient.id)&#10;&#10;          # Verify deletion was successful&#10;          assert result is True&#10;&#10;          # Verify patient no longer exists&#10;          retrieved_patient = await repository.get_by_id(created_patient.id)&#10;          assert retrieved_patient is None&#10;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24&#10;      @pytest.fixture&#10;      async def repository(self, db_session):&#10;          &quot;&quot;&quot;Create a patient repository with a real DB session.&quot;&quot;&quot;&#10;          return PatientRepository(db_session)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 104
      async def test_delete_patient(self, repository, sample_patient_data):
          """Test deleting a patient from the database."""
          # Create a patient first
          created_patient = await repository.create(sample_patient_data)

          # Delete the patient
          result = await repository.delete(created_patient.id)

          # Verify deletion was successful
          assert result is True

          # Verify patient no longer exists
          retrieved_patient = await repository.get_by_id(created_patient.id)
          assert retrieved_patient is None
file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24
      @pytest.fixture
      async def repository(self, db_session):
          """Create a patient repository with a real DB session."""
          return PatientRepository(db_session)
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24</error></testcase><testcase classname="app.tests.integration.test_patient_repository_int.TestPatientRepository" name="test_get_all_patients" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 119&#10;      async def test_get_all_patients(self, repository, sample_patient_data):&#10;          &quot;&quot;&quot;Test retrieving all patients from the database.&quot;&quot;&quot;&#10;          # Create multiple patients&#10;          patient_ids = []&#10;          for i in range(3):&#10;              data = sample_patient_data.copy()&#10;              data[&quot;id&quot;] = str(uuid.uuid4())&#10;              data[&quot;name&quot;] = f&quot;Test Patient {i}&quot;&#10;              data[&quot;email&quot;] = f&quot;patient{i}@example.com&quot;&#10;&#10;              patient = await repository.create(data)&#10;              patient_ids.append(patient.id)&#10;&#10;          # Get all patients&#10;          all_patients = await repository.get_all()&#10;&#10;          # Verify at least our test patients exist&#10;          assert len(all_patients) &gt;= 3&#10;          test_patients = [p for p in all_patients if p.id in patient_ids]&#10;          assert len(test_patients) == 3&#10;&#10;          # Clean up&#10;          for patient_id in patient_ids:&#10;              await repository.delete(patient_id)&#10;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24&#10;      @pytest.fixture&#10;      async def repository(self, db_session):&#10;          &quot;&quot;&quot;Create a patient repository with a real DB session.&quot;&quot;&quot;&#10;          return PatientRepository(db_session)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 119
      async def test_get_all_patients(self, repository, sample_patient_data):
          """Test retrieving all patients from the database."""
          # Create multiple patients
          patient_ids = []
          for i in range(3):
              data = sample_patient_data.copy()
              data["id"] = str(uuid.uuid4())
              data["name"] = f"Test Patient {i}"
              data["email"] = f"patient{i}@example.com"

              patient = await repository.create(data)
              patient_ids.append(patient.id)

          # Get all patients
          all_patients = await repository.get_all()

          # Verify at least our test patients exist
          assert len(all_patients) &gt;= 3
          test_patients = [p for p in all_patients if p.id in patient_ids]
          assert len(test_patients) == 3

          # Clean up
          for patient_id in patient_ids:
              await repository.delete(patient_id)
file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py, line 24
      @pytest.fixture
      async def repository(self, db_session):
          """Create a patient repository with a real DB session."""
          return PatientRepository(db_session)
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, repository, sample_patient_data, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_patient_repository_int.py:24</error></testcase><testcase classname="app.tests.integration.test_temporal_neurotransmitter_integration" name="test_temporal_service_with_xgboost_integration" time="0.001"><failure message="AttributeError: 'coroutine' object has no attribute 'generate_neurotransmitter_time_series'">temporal_service = &lt;coroutine object temporal_service at 0x7648ef8a67a0&gt;
xgboost_service = &lt;app.domain.services.enhanced_xgboost_service.EnhancedXGBoostService object at 0x7648ffa3a870&gt;
patient_id = UUID('6a204a85-d327-45fc-b8e6-2fe9a1d96a86')

    @pytest.mark.asyncio
    async def test_temporal_service_with_xgboost_integration(
        temporal_service: TemporalNeurotransmitterService,
        xgboost_service: EnhancedXGBoostService,
        patient_id: UUID
    ):
        """
        Test the integration between TemporalNeurotransmitterService and EnhancedXGBoostService.
    
        This test verifies that the temporal service correctly leverages XGBoost predictions
        to enhance treatment simulations.
        """
        # Set up - create a baseline sequence
&gt;       baseline_sequence_id = await temporal_service.generate_neurotransmitter_time_series(
            patient_id=patient_id,
            brain_region=BrainRegion.PREFRONTAL_CORTEX,
            neurotransmitter=Neurotransmitter.SEROTONIN,
            time_range_days=30,
            time_step_hours=6
        )
E       AttributeError: 'coroutine' object has no attribute 'generate_neurotransmitter_time_series'

app/tests/integration/test_temporal_neurotransmitter_integration.py:127: AttributeError</failure></testcase><testcase classname="app.tests.integration.test_temporal_neurotransmitter_integration" name="test_full_brain_region_coverage_with_visualization" time="0.001"><failure message="AttributeError: 'coroutine' object has no attribute 'get_cascade_visualization'">temporal_service = &lt;coroutine object temporal_service at 0x7648efd1c200&gt;
patient_id = UUID('ebe626f8-c9b7-4e60-b980-0353501462a6')

    @pytest.mark.asyncio
    async def test_full_brain_region_coverage_with_visualization(
        temporal_service: TemporalNeurotransmitterService,
        patient_id: UUID
    ):
        """
        Test complete horizontal coverage across brain regions with visualization.
    
        This test ensures that the service can generate visualizations for all brain regions.
        """
        # Test key brain regions with significant neurotransmitter involvement
        test_regions = [
            BrainRegion.PREFRONTAL_CORTEX,  # Executive function
            BrainRegion.AMYGDALA,           # Emotional processing
            BrainRegion.HIPPOCAMPUS,        # Memory
            BrainRegion.RAPHE_NUCLEI,       # Serotonin production
            BrainRegion.VENTRAL_TEGMENTAL_AREA  # Dopamine production
        ]
    
        visualization_data = {}
    
        # Generate visualization for each region
        for region in test_regions:
            # Create cascade visualization
&gt;           viz_data = await temporal_service.get_cascade_visualization(
                patient_id=patient_id,
                starting_region=region,
                neurotransmitter=Neurotransmitter.SEROTONIN,
                time_steps=5
            )
E           AttributeError: 'coroutine' object has no attribute 'get_cascade_visualization'

app/tests/integration/test_temporal_neurotransmitter_integration.py:216: AttributeError</failure></testcase><testcase classname="app.tests.integration.test_temporal_neurotransmitter_integration" name="test_full_neurotransmitter_coverage_with_treatment" time="0.001"><failure message="AttributeError: 'coroutine' object has no attribute 'simulate_treatment_response'">temporal_service = &lt;coroutine object temporal_service at 0x7648ef8a7680&gt;
patient_id = UUID('0e4e7167-ee92-4e85-96af-2dd3ca6aecbd')

    @pytest.mark.asyncio
    async def test_full_neurotransmitter_coverage_with_treatment(
        temporal_service: TemporalNeurotransmitterService,
        patient_id: UUID
    ):
        """
        Test complete horizontal coverage across neurotransmitters with treatment simulation.
    
        This test ensures that the service can simulate treatment effects for all neurotransmitters.
        """
        # Test major neurotransmitters involved in psychiatric conditions
        test_neurotransmitters = [
            Neurotransmitter.SEROTONIN,     # Depression, anxiety
            Neurotransmitter.DOPAMINE,      # Reward, addiction, psychosis
            Neurotransmitter.GABA,          # Anxiety, sleep
            Neurotransmitter.GLUTAMATE      # Learning, excitotoxicity
        ]
    
        treatment_results = {}
    
        # Simulate treatment for each neurotransmitter
        for nt in test_neurotransmitters:
            # Simulate treatment targeting this neurotransmitter
&gt;           response = await temporal_service.simulate_treatment_response(
                patient_id=patient_id,
                brain_region=BrainRegion.PREFRONTAL_CORTEX,
                target_neurotransmitter=nt,
                treatment_effect=0.5,
                simulation_days=7
            )
E           AttributeError: 'coroutine' object has no attribute 'simulate_treatment_response'

app/tests/integration/test_temporal_neurotransmitter_integration.py:272: AttributeError</failure></testcase><testcase classname="app.tests.integration.test_temporal_neurotransmitter_integration" name="test_api_integration_with_service" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_neurotransmitter_integration.py, line 303&#10;  @pytest.mark.asyncio&#10;  async def test_api_integration_with_service(&#10;      client: TestClient, # Use client fixture from conftest.py&#10;      mock_current_user,&#10;      # test_app fixture removed&#10;      temporal_service,&#10;      patient_id&#10;  ):&#10;      &quot;&quot;&quot;&#10;      Test API integration with the neurotransmitter service.&#10;&#10;      This test verifies that the API layer correctly integrates with the service layer.&#10;      &quot;&quot;&quot;&#10;      # Setup - patch dependencies to use our service instance&#10;      with mock_current_user, patch(&#10;          &quot;app.api.dependencies.services.get_temporal_neurotransmitter_service&quot;,&#10;          return_value=AsyncMock(return_value=temporal_service)&#10;      ):&#10;          # Test 1: Generate time series&#10;          time_series_response = client.post( # Use client fixture&#10;              &quot;/api/v1/temporal-neurotransmitter/time-series&quot;,&#10;              json={&#10;                  &quot;patient_id&quot;: str(patient_id),&#10;                  &quot;brain_region&quot;: BrainRegion.PREFRONTAL_CORTEX.value,&#10;                  &quot;neurotransmitter&quot;: Neurotransmitter.SEROTONIN.value,&#10;                  &quot;time_range_days&quot;: 14,&#10;                  &quot;time_step_hours&quot;: 6&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert time_series_response.status_code == 201&#10;          assert &quot;sequence_id&quot; in time_series_response.json()&#10;&#10;          # Test 2: Simulate treatment&#10;          treatment_response = client.post( # Use client fixture&#10;              &quot;/api/v1/temporal-neurotransmitter/simulate-treatment&quot;,&#10;              json={&#10;                  &quot;patient_id&quot;: str(patient_id),&#10;                  &quot;brain_region&quot;: BrainRegion.PREFRONTAL_CORTEX.value,&#10;                  &quot;target_neurotransmitter&quot;: Neurotransmitter.SEROTONIN.value,&#10;                  &quot;treatment_effect&quot;: 0.5,&#10;                  &quot;simulation_days&quot;: 14&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert treatment_response.status_code == 200&#10;          assert &quot;sequence_ids&quot; in treatment_response.json()&#10;&#10;          # Extract a sequence ID for visualization test&#10;          first_sequence_id = list(treatment_response.json()[&quot;sequence_ids&quot;].values())[0]&#10;&#10;          # Test 3: Get visualization data&#10;          viz_response = client.post( # Use client fixture&#10;              &quot;/api/v1/temporal-neurotransmitter/visualization-data&quot;,&#10;              json={&#10;                  &quot;sequence_id&quot;: first_sequence_id&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert viz_response.status_code == 200&#10;          assert &quot;time_points&quot; in viz_response.json()&#10;          assert &quot;features&quot; in viz_response.json()&#10;          assert &quot;values&quot; in viz_response.json()&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, db_session, doctest_namespace, event_loop, event_repository, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_current_user, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sequence_repository, session_mocker, temporal_service, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id, xgboost_service&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_neurotransmitter_integration.py:303&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_neurotransmitter_integration.py, line 303
  @pytest.mark.asyncio
  async def test_api_integration_with_service(
      client: TestClient, # Use client fixture from conftest.py
      mock_current_user,
      # test_app fixture removed
      temporal_service,
      patient_id
  ):
      """
      Test API integration with the neurotransmitter service.

      This test verifies that the API layer correctly integrates with the service layer.
      """
      # Setup - patch dependencies to use our service instance
      with mock_current_user, patch(
          "app.api.dependencies.services.get_temporal_neurotransmitter_service",
          return_value=AsyncMock(return_value=temporal_service)
      ):
          # Test 1: Generate time series
          time_series_response = client.post( # Use client fixture
              "/api/v1/temporal-neurotransmitter/time-series",
              json={
                  "patient_id": str(patient_id),
                  "brain_region": BrainRegion.PREFRONTAL_CORTEX.value,
                  "neurotransmitter": Neurotransmitter.SEROTONIN.value,
                  "time_range_days": 14,
                  "time_step_hours": 6
              }
          )

          # Verify response
          assert time_series_response.status_code == 201
          assert "sequence_id" in time_series_response.json()

          # Test 2: Simulate treatment
          treatment_response = client.post( # Use client fixture
              "/api/v1/temporal-neurotransmitter/simulate-treatment",
              json={
                  "patient_id": str(patient_id),
                  "brain_region": BrainRegion.PREFRONTAL_CORTEX.value,
                  "target_neurotransmitter": Neurotransmitter.SEROTONIN.value,
                  "treatment_effect": 0.5,
                  "simulation_days": 14
              }
          )

          # Verify response
          assert treatment_response.status_code == 200
          assert "sequence_ids" in treatment_response.json()

          # Extract a sequence ID for visualization test
          first_sequence_id = list(treatment_response.json()["sequence_ids"].values())[0]

          # Test 3: Get visualization data
          viz_response = client.post( # Use client fixture
              "/api/v1/temporal-neurotransmitter/visualization-data",
              json={
                  "sequence_id": first_sequence_id
              }
          )

          # Verify response
          assert viz_response.status_code == 200
          assert "time_points" in viz_response.json()
          assert "features" in viz_response.json()
          assert "values" in viz_response.json()
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, db_session, doctest_namespace, event_loop, event_repository, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_current_user, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sequence_repository, session_mocker, temporal_service, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id, xgboost_service
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_neurotransmitter_integration.py:303</error></testcase><testcase classname="app.tests.integration.test_temporal_wrapper" name="test_temporal_endpoints_integration" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_wrapper.py, line 18&#10;  @pytest.mark.asyncio&#10;  async def test_temporal_endpoints_integration(&#10;      client,&#10;      mock_current_user,&#10;      temporal_service: TemporalNeurotransmitterService,&#10;      patient_id: UUID&#10;  ):&#10;      &quot;&quot;&quot;&#10;      Test API integration with the neurotransmitter service.&#10;&#10;      This test verifies that the API layer correctly integrates with the service layer&#10;      without directly importing the router module.&#10;      &quot;&quot;&quot;&#10;      # Setup - patch dependencies to use our service instance&#10;      with mock_current_user, patch(&#10;          &quot;app.api.dependencies.services.get_temporal_neurotransmitter_service&quot;,&#10;          return_value=AsyncMock(return_value=temporal_service)&#10;      ):&#10;          # Test 1: Generate time series&#10;          time_series_response = client.post(&#10;              &quot;/api/v1/temporal-neurotransmitter/time-series&quot;,&#10;              json={&#10;                  &quot;patient_id&quot;: str(patient_id),&#10;                  &quot;brain_region&quot;: BrainRegion.PREFRONTAL_CORTEX.value,&#10;                  &quot;neurotransmitter&quot;: Neurotransmitter.SEROTONIN.value,&#10;                  &quot;time_range_days&quot;: 14,&#10;                  &quot;time_step_hours&quot;: 6&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert time_series_response.status_code == 201&#10;          assert &quot;sequence_id&quot; in time_series_response.json()&#10;&#10;          # Test 2: Simulate treatment&#10;          treatment_response = client.post(&#10;              &quot;/api/v1/temporal-neurotransmitter/simulate-treatment&quot;,&#10;              json={&#10;                  &quot;patient_id&quot;: str(patient_id),&#10;                  &quot;brain_region&quot;: BrainRegion.PREFRONTAL_CORTEX.value,&#10;                  &quot;target_neurotransmitter&quot;: Neurotransmitter.SEROTONIN.value,&#10;                  &quot;treatment_effect&quot;: 0.5,&#10;                  &quot;simulation_days&quot;: 14&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert treatment_response.status_code == 200&#10;          assert &quot;sequence_ids&quot; in treatment_response.json()&#10;&#10;          # Extract a sequence ID for visualization test&#10;          first_sequence_id = list(treatment_response.json()[&quot;sequence_ids&quot;].values())[0]&#10;&#10;          # Test 3: Get visualization data&#10;          viz_response = client.post(&#10;              &quot;/api/v1/temporal-neurotransmitter/visualization-data&quot;,&#10;              json={&#10;                  &quot;sequence_id&quot;: first_sequence_id&#10;              }&#10;          )&#10;&#10;          # Verify response&#10;          assert viz_response.status_code == 200&#10;          assert &quot;time_points&quot; in viz_response.json()&#10;          assert &quot;features&quot; in viz_response.json()&#10;          assert &quot;values&quot; in viz_response.json()&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_wrapper.py:18&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_wrapper.py, line 18
  @pytest.mark.asyncio
  async def test_temporal_endpoints_integration(
      client,
      mock_current_user,
      temporal_service: TemporalNeurotransmitterService,
      patient_id: UUID
  ):
      """
      Test API integration with the neurotransmitter service.

      This test verifies that the API layer correctly integrates with the service layer
      without directly importing the router module.
      """
      # Setup - patch dependencies to use our service instance
      with mock_current_user, patch(
          "app.api.dependencies.services.get_temporal_neurotransmitter_service",
          return_value=AsyncMock(return_value=temporal_service)
      ):
          # Test 1: Generate time series
          time_series_response = client.post(
              "/api/v1/temporal-neurotransmitter/time-series",
              json={
                  "patient_id": str(patient_id),
                  "brain_region": BrainRegion.PREFRONTAL_CORTEX.value,
                  "neurotransmitter": Neurotransmitter.SEROTONIN.value,
                  "time_range_days": 14,
                  "time_step_hours": 6
              }
          )

          # Verify response
          assert time_series_response.status_code == 201
          assert "sequence_id" in time_series_response.json()

          # Test 2: Simulate treatment
          treatment_response = client.post(
              "/api/v1/temporal-neurotransmitter/simulate-treatment",
              json={
                  "patient_id": str(patient_id),
                  "brain_region": BrainRegion.PREFRONTAL_CORTEX.value,
                  "target_neurotransmitter": Neurotransmitter.SEROTONIN.value,
                  "treatment_effect": 0.5,
                  "simulation_days": 14
              }
          )

          # Verify response
          assert treatment_response.status_code == 200
          assert "sequence_ids" in treatment_response.json()

          # Extract a sequence ID for visualization test
          first_sequence_id = list(treatment_response.json()["sequence_ids"].values())[0]

          # Test 3: Get visualization data
          viz_response = client.post(
              "/api/v1/temporal-neurotransmitter/visualization-data",
              json={
                  "sequence_id": first_sequence_id
              }
          )

          # Verify response
          assert viz_response.status_code == 200
          assert "time_points" in viz_response.json()
          assert "features" in viz_response.json()
          assert "values" in viz_response.json()
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/test_temporal_wrapper.py:18</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_analyze_actigraphy" time="0.001"><failure message="pydantic_core._pydantic_core.ValidationError: 2 validation errors for DeviceInfo&#10;manufacturer&#10;  Field required [type=missing, input_value={'device_type': 'fitbit',...mware_version': '1.2.3'}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;position&#10;  Field required [type=missing, input_value={'device_type': 'fitbit',...mware_version': '1.2.3'}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb64830&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ffaae3c0&gt;

    def test_analyze_actigraphy(self, client: TestClient):
        """Test the analyze actigraphy endpoint."""
        # Prepare test data
        patient_id = "test-patient-1"
        readings = [
            AccelerometerReading(
                timestamp=reading["timestamp"],
                x=reading["x"],
                y=reading["y"],
                z=reading["z"]
            )
            for reading in create_sample_readings(20)
        ]
        start_time = (datetime.now() - timedelta(hours=1)).isoformat()
        end_time = datetime.now().isoformat()
    
&gt;       device_info = DeviceInfo(
            device_type="fitbit",
            model="versa-3",
            firmware_version="1.2.3"
        )
E       pydantic_core._pydantic_core.ValidationError: 2 validation errors for DeviceInfo
E       manufacturer
E         Field required [type=missing, input_value={'device_type': 'fitbit',...mware_version': '1.2.3'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       position
E         Field required [type=missing, input_value={'device_type': 'fitbit',...mware_version': '1.2.3'}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing

app/tests/integration/api/test_actigraphy_api.py:166: ValidationError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_get_embeddings" time="0.010"><failure message="assert 404 == 200&#10; +  where 404 = &lt;Response [404 Not Found]&gt;.status_code&#10; +  and   200 = status.HTTP_200_OK">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb64140&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ffad8e00&gt;

    def test_get_embeddings(self, client: TestClient):
        """Test the get embeddings endpoint."""
        # Prepare test data
        patient_id = "test-patient-1"
        readings = [
            AccelerometerReading(
                timestamp=reading["timestamp"],
                x=reading["x"],
                y=reading["y"],
                z=reading["z"]
            )
            for reading in create_sample_readings(20)
        ]
        start_time = (datetime.now() - timedelta(hours=1)).isoformat()
        end_time = datetime.now().isoformat()
    
        request_data = {
            "patient_id": patient_id,
            "readings": [reading.model_dump() for reading in readings],
            "start_time": start_time,
            "end_time": end_time,
            "sampling_rate_hz": 10.0
        }
    
        # Make request
        response = client.post(
            "/api/actigraphy/embeddings",
            json=request_data,
            headers={"Authorization": "Bearer test-token"}
        )
    
        # Verify response
&gt;       assert response.status_code == status.HTTP_200_OK
E       assert 404 == 200
E        +  where 404 = &lt;Response [404 Not Found]&gt;.status_code
E        +  and   200 = status.HTTP_200_OK

app/tests/integration/api/test_actigraphy_api.py:242: AssertionError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_get_analysis_by_id" time="0.001"><failure message="AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb65a60&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ef358620&gt;
mock_pat_service = &lt;app.core.services.ml.pat.mock.MockPATService object at 0x7648ffad9ca0&gt;

    def test_get_analysis_by_id(self, client: TestClient, mock_pat_service: MockPATService):
        """Test retrieving an analysis by ID."""
        # First create an analysis
        patient_id = "test-patient-1"
        readings = create_sample_readings(20)
        start_time = datetime.now() - timedelta(hours=1)
        end_time = datetime.now()
    
&gt;       analysis = mock_pat_service.analyze_actigraphy(
            patient_id=patient_id,
            readings=readings,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            sampling_rate_hz=10.0,
            device_info={"device_type": "fitbit", "model": "versa-3"},
            analysis_types=["sleep_quality"]
        )
E       AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'

app/tests/integration/api/test_actigraphy_api.py:260: AttributeError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_get_patient_analyses" time="0.001"><failure message="AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb65c10&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ef359d30&gt;
mock_pat_service = &lt;app.core.services.ml.pat.mock.MockPATService object at 0x7648ef3594f0&gt;

    def test_get_patient_analyses(self, client: TestClient, mock_pat_service: MockPATService):
        """Test retrieving analyses for a patient."""
        # First create multiple analyses for the same patient
        patient_id = "test-patient-2"
    
        for i in range(3):
            readings = create_sample_readings(20)
            start_time = datetime.now() - timedelta(hours=i+1)
            end_time = datetime.now() - timedelta(hours=i)
    
&gt;           mock_pat_service.analyze_actigraphy(
                patient_id=patient_id,
                readings=readings,
                start_time=start_time.isoformat(),
                end_time=end_time.isoformat(),
                sampling_rate_hz=10.0,
                device_info={"device_type": "fitbit", "model": "versa-3"},
                analysis_types=["sleep_quality", "activity_levels"]
            )
E           AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'

app/tests/integration/api/test_actigraphy_api.py:295: AttributeError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_get_model_info" time="0.003" /><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_integrate_with_digital_twin" time="0.001"><failure message="AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb65f70&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ef3a9190&gt;
mock_pat_service = &lt;app.core.services.ml.pat.mock.MockPATService object at 0x7648ef35bec0&gt;

    def test_integrate_with_digital_twin(self, client: TestClient, mock_pat_service: MockPATService):
        """Test integrating actigraphy analysis with a digital twin."""
        # Prepare test data
        patient_id = "test-patient-1"
        profile_id = "test-profile-1"
    
        # First create an analysis
        readings = create_sample_readings(20)
        start_time = datetime.now() - timedelta(hours=1)
        end_time = datetime.now()
    
&gt;       analysis = mock_pat_service.analyze_actigraphy(
            patient_id=patient_id,
            readings=readings,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            sampling_rate_hz=10.0,
            device_info={"device_type": "fitbit", "model": "versa-3"},
            analysis_types=["sleep_quality", "activity_levels"]
        )
E       AttributeError: 'MockPATService' object has no attribute 'analyze_actigraphy'

app/tests/integration/api/test_actigraphy_api.py:354: AttributeError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api.TestActigraphyAPI" name="test_unauthorized_access" time="0.002"><failure message="assert 404 == 403&#10; +  where 404 = &lt;Response [404 Not Found]&gt;.status_code&#10; +  and   403 = status.HTTP_403_FORBIDDEN">self = &lt;test_actigraphy_api.TestActigraphyAPI object at 0x7648efb66120&gt;
client = &lt;starlette.testclient.TestClient object at 0x7648ef35a960&gt;

    def test_unauthorized_access(self, client: TestClient):
        """Test unauthorized access to the API."""
        # Make request without token
        response = client.get(
            "/api/actigraphy/model-info"
        )
    
        # Verify response
&gt;       assert response.status_code == status.HTTP_403_FORBIDDEN
E       assert 404 == 403
E        +  where 404 = &lt;Response [404 Not Found]&gt;.status_code
E        +  and   403 = status.HTTP_403_FORBIDDEN

app/tests/integration/api/test_actigraphy_api.py:395: AssertionError</failure></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_analyze_actigraphy" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 82&#10;      def test_analyze_actigraphy(self, client: TestClient, auth_headers, actigraphy_data): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:82&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 82
      def test_analyze_actigraphy(self, client: TestClient, auth_headers, actigraphy_data): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:82</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_get_actigraphy_embeddings" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 100&#10;      def test_get_actigraphy_embeddings(self, client: TestClient, auth_headers, actigraphy_data): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:100&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 100
      def test_get_actigraphy_embeddings(self, client: TestClient, auth_headers, actigraphy_data): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:100</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_get_analysis_by_id" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 127&#10;      def test_get_analysis_by_id(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:127&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 127
      def test_get_analysis_by_id(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:127</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_get_patient_analyses" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 154&#10;      def test_get_patient_analyses(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:154&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 154
      def test_get_patient_analyses(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:154</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_get_model_info" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 182&#10;      def test_get_model_info(self, client: TestClient, auth_headers): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:182&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 182
      def test_get_model_info(self, client: TestClient, auth_headers): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:182</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_integrate_with_digital_twin" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 196&#10;      def test_integrate_with_digital_twin(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:196&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 196
      def test_integrate_with_digital_twin(self, client: TestClient, auth_headers, mock_pat_service): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:196</error></testcase><testcase classname="app.tests.integration.api.test_actigraphy_api_integration.TestActigraphyAPI" name="test_unauthorized_access" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 233&#10;      def test_unauthorized_access(self, client: TestClient, actigraphy_data): # Use client fixture&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:233&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py, line 233
      def test_unauthorized_access(self, client: TestClient, actigraphy_data): # Use client fixture
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, actigraphy_data, anyio_backend, anyio_backend_name, anyio_backend_options, auth_headers, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_actigraphy_api_integration.py:233</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_risk_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 298&#10;      def test_predict_risk_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:298&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 298
      def test_predict_risk_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:298</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_risk_validation_error" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 350&#10;      def test_predict_risk_validation_error(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:350&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 350
      def test_predict_risk_validation_error(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:350</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_risk_phi_detection" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 379&#10;      def test_predict_risk_phi_detection(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:379&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 379
      def test_predict_risk_phi_detection(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:379</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_risk_unauthorized" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 413&#10;      def test_predict_risk_unauthorized(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:413&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 413
      def test_predict_risk_unauthorized(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:413</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_treatment_response_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 435&#10;      def test_predict_treatment_response_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:435&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 435
      def test_predict_treatment_response_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:435</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_predict_outcome_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 501&#10;      def test_predict_outcome_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:501&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 501
      def test_predict_outcome_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:501</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_get_feature_importance_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 577&#10;      def test_get_feature_importance_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:577&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 577
      def test_get_feature_importance_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:577</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_get_feature_importance_not_found" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 634&#10;      def test_get_feature_importance_not_found(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:634&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 634
      def test_get_feature_importance_not_found(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:634</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_integrate_with_digital_twin_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 663&#10;      def test_integrate_with_digital_twin_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:663&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 663
      def test_integrate_with_digital_twin_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:663</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_get_model_info_success" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 714&#10;      def test_get_model_info_success(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:714&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 714
      def test_get_model_info_success(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:714</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_get_model_info_not_found" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 762&#10;      def test_get_model_info_not_found(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:762&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 762
      def test_get_model_info_not_found(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:762</error></testcase><testcase classname="app.tests.integration.api.test_xgboost_api_integration.TestXGBoostAPIIntegration" name="test_service_unavailable" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 787&#10;      def test_service_unavailable(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:787&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py, line 787
      def test_service_unavailable(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_xgboost_service, mocker, module_mocker, monkeypatch, no_cover, package_mocker, patient_auth_headers, provider_auth_headers, psychiatrist_auth_headers, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_outcome_prediction_data, valid_risk_prediction_data, valid_treatment_response_data, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/test_xgboost_api_integration.py:787</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_unauthenticated_access" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 127&#10;  def test_unauthenticated_access(client: TestClient) -&gt; None:&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:127&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 127
  def test_unauthenticated_access(client: TestClient) -&gt; None:
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:127</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_authorized_access" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 141&#10;  def test_authorized_access(client: TestClient, patient_token: str) -&gt; None:&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:141&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 141
  def test_authorized_access(client: TestClient, patient_token: str) -&gt; None:
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:141</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_input_validation" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 158&#10;  def test_input_validation(client: TestClient, patient_token: str) -&gt; None:&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:158&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 158
  def test_input_validation(client: TestClient, patient_token: str) -&gt; None:
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:158</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_phi_data_sanitization" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 188&#10;  def test_phi_data_sanitization(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:188&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 188
  def test_phi_data_sanitization(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:188</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_role_based_access_control" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 244&#10;  def test_role_based_access_control(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:244&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 244
  def test_role_based_access_control(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:244</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_hipaa_audit_logging" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 308&#10;  def test_hipaa_audit_logging(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:308&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 308
  def test_hipaa_audit_logging(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:308</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_secure_data_transmission" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 378&#10;  def test_secure_data_transmission(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:378&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 378
  def test_secure_data_transmission(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:378</error></testcase><testcase classname="app.tests.integration.api.endpoints.test_actigraphy_endpoints" name="test_api_response_structure" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 473&#10;  def test_api_response_structure(&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:473&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py, line 473
  def test_api_response_structure(
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, admin_token, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_pat, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pat_storage, patient_token, provider_token, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_device_info, sample_readings, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/api/endpoints/test_actigraphy_endpoints.py:473</error></testcase><testcase classname="app.tests.integration.infrastructure.persistence.test_patient_encryption_integration.TestPatientEncryptionIntegration" name="test_phi_encrypted_in_database" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 58&#10;      @pytest.mark.asyncio&#10;      async def test_phi_encrypted_in_database(self, db_session: AsyncSession, sample_patient):&#10;          &quot;&quot;&quot;Test that PHI is stored encrypted in the database.&quot;&quot;&quot;&#10;          # Create a real encryption service&#10;          encryption_service = EncryptionService()&#10;&#10;          # Convert domain entity to model and save to database&#10;          patient_model = PatientModel.from_domain(sample_patient)&#10;          db_session.add(patient_model)&#10;          await db_session.commit()&#10;&#10;          # Verify patient was saved&#10;          assert patient_model.id is not None&#10;&#10;          # Get raw database data to verify encryption&#10;          query = text(&#10;              &quot;SELECT first_name, last_name, date_of_birth, email, phone, address_line1 &quot;&#10;              &quot;FROM patients WHERE id = :id&quot;&#10;          )&#10;          result = await db_session.execute(query, {&quot;id&quot;: str(patient_model.id)})&#10;          row = result.fetchone()&#10;&#10;          # Verify PHI data is stored encrypted (check that it doesn't match plaintext)&#10;          assert row.first_name != sample_patient.first_name&#10;          assert row.last_name != sample_patient.last_name&#10;          assert row.date_of_birth != sample_patient.date_of_birth.isoformat()&#10;          assert row.email != sample_patient.email&#10;          assert row.phone != sample_patient.phone&#10;          assert row.address_line1 != sample_patient.address.line1&#10;&#10;          # Verify we can decrypt the data correctly&#10;          decrypted_first_name = encryption_service.decrypt(row.first_name)&#10;          decrypted_email = encryption_service.decrypt(row.email)&#10;&#10;          assert decrypted_first_name == sample_patient.first_name&#10;          assert decrypted_email == sample_patient.email&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:58&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 58
      @pytest.mark.asyncio
      async def test_phi_encrypted_in_database(self, db_session: AsyncSession, sample_patient):
          """Test that PHI is stored encrypted in the database."""
          # Create a real encryption service
          encryption_service = EncryptionService()

          # Convert domain entity to model and save to database
          patient_model = PatientModel.from_domain(sample_patient)
          db_session.add(patient_model)
          await db_session.commit()

          # Verify patient was saved
          assert patient_model.id is not None

          # Get raw database data to verify encryption
          query = text(
              "SELECT first_name, last_name, date_of_birth, email, phone, address_line1 "
              "FROM patients WHERE id = :id"
          )
          result = await db_session.execute(query, {"id": str(patient_model.id)})
          row = result.fetchone()

          # Verify PHI data is stored encrypted (check that it doesn't match plaintext)
          assert row.first_name != sample_patient.first_name
          assert row.last_name != sample_patient.last_name
          assert row.date_of_birth != sample_patient.date_of_birth.isoformat()
          assert row.email != sample_patient.email
          assert row.phone != sample_patient.phone
          assert row.address_line1 != sample_patient.address.line1

          # Verify we can decrypt the data correctly
          decrypted_first_name = encryption_service.decrypt(row.first_name)
          decrypted_email = encryption_service.decrypt(row.email)

          assert decrypted_first_name == sample_patient.first_name
          assert decrypted_email == sample_patient.email
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:58</error></testcase><testcase classname="app.tests.integration.infrastructure.persistence.test_patient_encryption_integration.TestPatientEncryptionIntegration" name="test_phi_decrypted_in_repository" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 95&#10;      @pytest.mark.asyncio&#10;      async def test_phi_decrypted_in_repository(self, db_session: AsyncSession, sample_patient):&#10;          &quot;&quot;&quot;Test that PHI is automatically decrypted when retrieved through repository.&quot;&quot;&quot;&#10;          # Save encrypted patient to database&#10;          patient_model = PatientModel.from_domain(sample_patient)&#10;          db_session.add(patient_model)&#10;          await db_session.commit()&#10;          patient_id = patient_model.id&#10;&#10;          # Clear session cache to ensure we're retrieving from DB&#10;          await db_session.close()&#10;&#10;          # Retrieve patient from database&#10;          retrieved_patient_model = await db_session.get(PatientModel, patient_id)&#10;          retrieved_patient = retrieved_patient_model.to_domain()&#10;&#10;          # Verify PHI fields are correctly decrypted&#10;          assert retrieved_patient.first_name == sample_patient.first_name&#10;          assert retrieved_patient.last_name == sample_patient.last_name&#10;          assert retrieved_patient.date_of_birth == sample_patient.date_of_birth&#10;          assert retrieved_patient.email == sample_patient.email&#10;          assert retrieved_patient.phone == sample_patient.phone&#10;&#10;          # Verify complex PHI objects are decrypted&#10;          assert retrieved_patient.address.line1 == sample_patient.address.line1&#10;          assert retrieved_patient.address.city == sample_patient.address.city&#10;&#10;          assert retrieved_patient.emergency_contact.name == sample_patient.emergency_contact.name&#10;          assert retrieved_patient.emergency_contact.phone == sample_patient.emergency_contact.phone&#10;&#10;          # Verify insurance_info dictionary&#10;          assert retrieved_patient.insurance_info[&quot;provider&quot;] == sample_patient.insurance_info[&quot;provider&quot;]&#10;          assert retrieved_patient.insurance_info[&quot;policy_number&quot;] == sample_patient.insurance_info[&quot;policy_number&quot;]&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:95&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 95
      @pytest.mark.asyncio
      async def test_phi_decrypted_in_repository(self, db_session: AsyncSession, sample_patient):
          """Test that PHI is automatically decrypted when retrieved through repository."""
          # Save encrypted patient to database
          patient_model = PatientModel.from_domain(sample_patient)
          db_session.add(patient_model)
          await db_session.commit()
          patient_id = patient_model.id

          # Clear session cache to ensure we're retrieving from DB
          await db_session.close()

          # Retrieve patient from database
          retrieved_patient_model = await db_session.get(PatientModel, patient_id)
          retrieved_patient = retrieved_patient_model.to_domain()

          # Verify PHI fields are correctly decrypted
          assert retrieved_patient.first_name == sample_patient.first_name
          assert retrieved_patient.last_name == sample_patient.last_name
          assert retrieved_patient.date_of_birth == sample_patient.date_of_birth
          assert retrieved_patient.email == sample_patient.email
          assert retrieved_patient.phone == sample_patient.phone

          # Verify complex PHI objects are decrypted
          assert retrieved_patient.address.line1 == sample_patient.address.line1
          assert retrieved_patient.address.city == sample_patient.address.city

          assert retrieved_patient.emergency_contact.name == sample_patient.emergency_contact.name
          assert retrieved_patient.emergency_contact.phone == sample_patient.emergency_contact.phone

          # Verify insurance_info dictionary
          assert retrieved_patient.insurance_info["provider"] == sample_patient.insurance_info["provider"]
          assert retrieved_patient.insurance_info["policy_number"] == sample_patient.insurance_info["policy_number"]
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:95</error></testcase><testcase classname="app.tests.integration.infrastructure.persistence.test_patient_encryption_integration.TestPatientEncryptionIntegration" name="test_encryption_error_handling" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 129&#10;      @pytest.mark.asyncio&#10;      async def test_encryption_error_handling(self, db_session: AsyncSession):&#10;          &quot;&quot;&quot;Test that encryption/decryption errors are handled gracefully.&quot;&quot;&quot;&#10;          # Create patient with an ID that can be referenced in logs without exposing PHI&#10;          patient_id = uuid.uuid4()&#10;          patient = Patient(&#10;              id=patient_id,&#10;              first_name=&quot;Test&quot;,&#10;              last_name=&quot;Patient&quot;,&#10;              date_of_birth=date(1990, 1, 1),&#10;              email=&quot;test@example.com&quot;,&#10;              phone=&quot;555-555-5555&quot;,&#10;              address=None,  # Test with minimal data&#10;              emergency_contact=None,&#10;              insurance=None,&#10;              active=True,&#10;              created_by=None&#10;          )&#10;&#10;          # Save to database&#10;          patient_model = PatientModel.from_domain(patient)&#10;          db_session.add(patient_model)&#10;          await db_session.commit()&#10;&#10;          # Manually corrupt the encrypted data to simulate decryption error&#10;          # Note: In a real test, you might use a mock or patch to force a decryption error&#10;          await db_session.execute(&#10;              text(&quot;UPDATE patients SET first_name = 'CORRUPTED_DATA' WHERE id = :id&quot;),&#10;              {&quot;id&quot;: str(patient_id)}&#10;          )&#10;          await db_session.commit()&#10;&#10;          # Retrieve patient - this should handle the decryption error gracefully&#10;          # (Error should be logged but not expose PHI or crash the application)&#10;          retrieved_model = await db_session.get(PatientModel, patient_id)&#10;          retrieved_patient = retrieved_model.to_domain()&#10;&#10;          # The decryption failure for first_name should result in None rather than crashing&#10;          assert retrieved_patient.id == patient_id  # ID should still match&#10;          assert retrieved_patient.first_name is None  # Failed decryption should return None&#10;          assert retrieved_patient.last_name == patient.last_name  # Other fields should be fine&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:129&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py, line 129
      @pytest.mark.asyncio
      async def test_encryption_error_handling(self, db_session: AsyncSession):
          """Test that encryption/decryption errors are handled gracefully."""
          # Create patient with an ID that can be referenced in logs without exposing PHI
          patient_id = uuid.uuid4()
          patient = Patient(
              id=patient_id,
              first_name="Test",
              last_name="Patient",
              date_of_birth=date(1990, 1, 1),
              email="test@example.com",
              phone="555-555-5555",
              address=None,  # Test with minimal data
              emergency_contact=None,
              insurance=None,
              active=True,
              created_by=None
          )

          # Save to database
          patient_model = PatientModel.from_domain(patient)
          db_session.add(patient_model)
          await db_session.commit()

          # Manually corrupt the encrypted data to simulate decryption error
          # Note: In a real test, you might use a mock or patch to force a decryption error
          await db_session.execute(
              text("UPDATE patients SET first_name = 'CORRUPTED_DATA' WHERE id = :id"),
              {"id": str(patient_id)}
          )
          await db_session.commit()

          # Retrieve patient - this should handle the decryption error gracefully
          # (Error should be logged but not expose PHI or crash the application)
          retrieved_model = await db_session.get(PatientModel, patient_id)
          retrieved_patient = retrieved_model.to_domain()

          # The decryption failure for first_name should result in None rather than crashing
          assert retrieved_patient.id == patient_id  # ID should still match
          assert retrieved_patient.first_name is None  # Failed decryption should return None
          assert retrieved_patient.last_name == patient.last_name  # Other fields should be fine
E       fixture 'db_session' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_patient, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/integration/infrastructure/persistence/test_patient_encryption_integration.py:129</error></testcase><testcase classname="app.tests.integration.security.test_phi_sanitization_integration.TestPHISanitizationIntegration" name="test_patient_phi_database_encryption" time="0.001"><failure message="AttributeError: 'Table' object has no attribute 'from_domain'">self = &lt;test_phi_sanitization_integration.TestPHISanitizationIntegration object at 0x7648ef9091c0&gt;
test_patient = &lt;coroutine object test_patient at 0x7648ffadd5a0&gt;

    @pytest.mark.asyncio
    async def test_patient_phi_database_encryption(self, test_patient: Patient):
        """Test that PHI is encrypted in database and decrypted when retrieved."""
        # Convert to model (encrypts PHI)
&gt;       patient_model = PatientModel.from_domain(test_patient)
E       AttributeError: 'Table' object has no attribute 'from_domain'

app/tests/integration/security/test_phi_sanitization_integration.py:97: AttributeError</failure></testcase><testcase classname="app.tests.integration.security.test_phi_sanitization_integration.TestPHISanitizationIntegration" name="test_phi_sanitization_in_logs" time="0.004"><failure message="AttributeError: 'coroutine' object has no attribute 'id'">self = &lt;test_phi_sanitization_integration.TestPHISanitizationIntegration object at 0x7648ef909310&gt;
test_patient = &lt;coroutine object test_patient at 0x7648ef9cfcd0&gt;
log_capture = &lt;_io.StringIO object at 0x7648ffac6d40&gt;

    @pytest.mark.asyncio
    async def test_phi_sanitization_in_logs(self, test_patient: Patient, log_capture: StringIO):
        """Test that PHI is properly sanitized in logs."""
        # Set up logger
        logger = get_sanitized_logger("test.phi.integration") # Use correct function
    
        # Log some PHI
        logger.info(
            f"Processing patient record",
            {
&gt;               "patient_id": str(test_patient.id),
                "email": test_patient.email,
                "phone": test_patient.phone,
                "dob": str(test_patient.date_of_birth)
            }
        )
E       AttributeError: 'coroutine' object has no attribute 'id'

app/tests/integration/security/test_phi_sanitization_integration.py:147: AttributeError</failure></testcase><testcase classname="app.tests.integration.security.test_phi_sanitization_integration.TestPHISanitizationIntegration" name="test_phi_sanitization_during_errors" time="0.001"><failure message="AttributeError: 'coroutine' object has no attribute 'first_name'">self = &lt;test_phi_sanitization_integration.TestPHISanitizationIntegration object at 0x7648ef909490&gt;
test_patient = &lt;coroutine object test_patient at 0x7648ffadddf0&gt;
log_capture = &lt;_io.StringIO object at 0x7648ffac62c0&gt;

    @pytest.mark.asyncio
    async def test_phi_sanitization_during_errors(self, test_patient: Patient, log_capture: StringIO):
        """Test that PHI is sanitized even during error handling."""
        # Set up logger
        logger = get_sanitized_logger("test.phi.error") # Use correct function
    
        try:
            # Simulate an error with PHI in the message
            error_message = (
&gt;               f"Error processing patient {test_patient.first_name} {test_patient.last_name} "
                f"(DOB: {test_patient.date_of_birth}, Email: {test_patient.email})"
            )
E           AttributeError: 'coroutine' object has no attribute 'first_name'

app/tests/integration/security/test_phi_sanitization_integration.py:174: AttributeError</failure></testcase><testcase classname="app.tests.integration.security.test_phi_sanitization_integration.TestPHISanitizationIntegration" name="test_cross_module_phi_protection" time="0.001"><failure message="AttributeError: 'Table' object has no attribute 'from_domain'">self = &lt;test_phi_sanitization_integration.TestPHISanitizationIntegration object at 0x7648ef909610&gt;
test_patient = &lt;coroutine object test_patient at 0x7648ffade180&gt;
log_capture = &lt;_io.StringIO object at 0x7648ffac4c40&gt;

    @pytest.mark.asyncio
    async def test_cross_module_phi_protection(self, test_patient: Patient, log_capture: StringIO):
        """Test PHI protection across module boundaries."""
        # This test simulates a full pipeline that processes patient data
    
        # Convert to model (simulating data access layer)
&gt;       patient_model = PatientModel.from_domain(test_patient)
E       AttributeError: 'Table' object has no attribute 'from_domain'

app/tests/integration/security/test_phi_sanitization_integration.py:206: AttributeError</failure></testcase><testcase classname="app.tests.integration.security.test_security_boundary.TestSecurityBoundary" name="test_complete_auth_flow" time="0.000"><error message="failed on setup with &quot;AttributeError: 'Settings' object has no attribute 'security'&quot;">@pytest.fixture
    def security_components():
        """
        Create the core security components needed for testing.
    
        Returns:
            Tuple of (jwt_handler, password_handler, role_manager)
        """
        jwt_handler = JWTHandler(
            secret_key="testkey12345678901234567890123456789",
            algorithm="HS256",
            access_token_expire_minutes=15
        )
    
&gt;       password_handler = PasswordHandler()

app/tests/integration/security/test_security_boundary.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
app/infrastructure/security/password/password_handler.py:34: in __init__
    bcrypt__rounds=settings.security.PASSWORD_SALT_ROUNDS,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Settings(PROJECT_NAME='Novamind Digital Twin', API_VERSION='v1', API_PREFIX='/api/v1', APP_DESCRIPTION='Advanced psych...lse, SECURE_HSTS_SECONDS=0, CACHE_TTL_SECONDS=60, DEFAULT_BRAIN_VIEW='sagittal', NEUROTRANSMITTER_SIMULATION_STEPS=100)
item = 'security'

    def __getattr__(self, item: str) -&gt; Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
&gt;                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Settings' object has no attribute 'security'

/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:856: AttributeError</error></testcase><testcase classname="app.tests.integration.security.test_security_boundary.TestSecurityBoundary" name="test_provider_access_scope" time="0.000"><error message="failed on setup with &quot;AttributeError: 'Settings' object has no attribute 'security'&quot;">@pytest.fixture
    def security_components():
        """
        Create the core security components needed for testing.
    
        Returns:
            Tuple of (jwt_handler, password_handler, role_manager)
        """
        jwt_handler = JWTHandler(
            secret_key="testkey12345678901234567890123456789",
            algorithm="HS256",
            access_token_expire_minutes=15
        )
    
&gt;       password_handler = PasswordHandler()

app/tests/integration/security/test_security_boundary.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
app/infrastructure/security/password/password_handler.py:34: in __init__
    bcrypt__rounds=settings.security.PASSWORD_SALT_ROUNDS,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Settings(PROJECT_NAME='Novamind Digital Twin', API_VERSION='v1', API_PREFIX='/api/v1', APP_DESCRIPTION='Advanced psych...lse, SECURE_HSTS_SECONDS=0, CACHE_TTL_SECONDS=60, DEFAULT_BRAIN_VIEW='sagittal', NEUROTRANSMITTER_SIMULATION_STEPS=100)
item = 'security'

    def __getattr__(self, item: str) -&gt; Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
&gt;                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Settings' object has no attribute 'security'

/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:856: AttributeError</error></testcase><testcase classname="app.tests.integration.security.test_security_boundary.TestSecurityBoundary" name="test_token_expiration_security" time="0.000"><error message="failed on setup with &quot;AttributeError: 'Settings' object has no attribute 'security'&quot;">@pytest.fixture
    def security_components():
        """
        Create the core security components needed for testing.
    
        Returns:
            Tuple of (jwt_handler, password_handler, role_manager)
        """
        jwt_handler = JWTHandler(
            secret_key="testkey12345678901234567890123456789",
            algorithm="HS256",
            access_token_expire_minutes=15
        )
    
&gt;       password_handler = PasswordHandler()

app/tests/integration/security/test_security_boundary.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
app/infrastructure/security/password/password_handler.py:34: in __init__
    bcrypt__rounds=settings.security.PASSWORD_SALT_ROUNDS,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Settings(PROJECT_NAME='Novamind Digital Twin', API_VERSION='v1', API_PREFIX='/api/v1', APP_DESCRIPTION='Advanced psych...lse, SECURE_HSTS_SECONDS=0, CACHE_TTL_SECONDS=60, DEFAULT_BRAIN_VIEW='sagittal', NEUROTRANSMITTER_SIMULATION_STEPS=100)
item = 'security'

    def __getattr__(self, item: str) -&gt; Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
&gt;                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Settings' object has no attribute 'security'

/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:856: AttributeError</error></testcase><testcase classname="app.tests.integration.security.test_security_boundary.TestSecurityBoundary" name="test_password_strength_enforcement" time="0.000"><error message="failed on setup with &quot;AttributeError: 'Settings' object has no attribute 'security'&quot;">@pytest.fixture
    def security_components():
        """
        Create the core security components needed for testing.
    
        Returns:
            Tuple of (jwt_handler, password_handler, role_manager)
        """
        jwt_handler = JWTHandler(
            secret_key="testkey12345678901234567890123456789",
            algorithm="HS256",
            access_token_expire_minutes=15
        )
    
&gt;       password_handler = PasswordHandler()

app/tests/integration/security/test_security_boundary.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
app/infrastructure/security/password/password_handler.py:34: in __init__
    bcrypt__rounds=settings.security.PASSWORD_SALT_ROUNDS,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Settings(PROJECT_NAME='Novamind Digital Twin', API_VERSION='v1', API_PREFIX='/api/v1', APP_DESCRIPTION='Advanced psych...lse, SECURE_HSTS_SECONDS=0, CACHE_TTL_SECONDS=60, DEFAULT_BRAIN_VIEW='sagittal', NEUROTRANSMITTER_SIMULATION_STEPS=100)
item = 'security'

    def __getattr__(self, item: str) -&gt; Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
&gt;                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Settings' object has no attribute 'security'

/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:856: AttributeError</error></testcase><testcase classname="app.tests.integration.security.test_security_boundary.TestSecurityBoundary" name="test_admin_special_privileges" time="0.000"><error message="failed on setup with &quot;AttributeError: 'Settings' object has no attribute 'security'&quot;">@pytest.fixture
    def security_components():
        """
        Create the core security components needed for testing.
    
        Returns:
            Tuple of (jwt_handler, password_handler, role_manager)
        """
        jwt_handler = JWTHandler(
            secret_key="testkey12345678901234567890123456789",
            algorithm="HS256",
            access_token_expire_minutes=15
        )
    
&gt;       password_handler = PasswordHandler()

app/tests/integration/security/test_security_boundary.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
app/infrastructure/security/password/password_handler.py:34: in __init__
    bcrypt__rounds=settings.security.PASSWORD_SALT_ROUNDS,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Settings(PROJECT_NAME='Novamind Digital Twin', API_VERSION='v1', API_PREFIX='/api/v1', APP_DESCRIPTION='Advanced psych...lse, SECURE_HSTS_SECONDS=0, CACHE_TTL_SECONDS=60, DEFAULT_BRAIN_VIEW='sagittal', NEUROTRANSMITTER_SIMULATION_STEPS=100)
item = 'security'

    def __getattr__(self, item: str) -&gt; Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
&gt;                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Settings' object has no attribute 'security'

/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:856: AttributeError</error></testcase><testcase classname="app.tests.api.integration.test_xgboost_integration.TestXGBoostIntegration" name="test_risk_prediction_flow" time="0.001"><error message="failed on setup with &quot;AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'&quot;">@pytest.fixture(autouse=True)
    def mock_auth_dependencies():
        """Mock authentication dependencies for all tests."""
&gt;       with patch("app.api.dependencies.auth.get_current_clinician") as mock_get_clinician, \
             patch("app.api.dependencies.auth.verify_patient_access") as mock_verify_access:

app/tests/api/integration/test_xgboost_integration.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;unittest.mock._patch object at 0x7648ffad8b90&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'

/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1428: AttributeError</error></testcase><testcase classname="app.tests.api.integration.test_xgboost_integration.TestXGBoostIntegration" name="test_treatment_comparison_flow" time="0.000"><error message="failed on setup with &quot;AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'&quot;">@pytest.fixture(autouse=True)
    def mock_auth_dependencies():
        """Mock authentication dependencies for all tests."""
&gt;       with patch("app.api.dependencies.auth.get_current_clinician") as mock_get_clinician, \
             patch("app.api.dependencies.auth.verify_patient_access") as mock_verify_access:

app/tests/api/integration/test_xgboost_integration.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;unittest.mock._patch object at 0x7648ef35ae10&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'

/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1428: AttributeError</error></testcase><testcase classname="app.tests.api.integration.test_xgboost_integration.TestXGBoostIntegration" name="test_model_info_flow" time="0.000"><error message="failed on setup with &quot;AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'&quot;">@pytest.fixture(autouse=True)
    def mock_auth_dependencies():
        """Mock authentication dependencies for all tests."""
&gt;       with patch("app.api.dependencies.auth.get_current_clinician") as mock_get_clinician, \
             patch("app.api.dependencies.auth.verify_patient_access") as mock_verify_access:

app/tests/api/integration/test_xgboost_integration.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;unittest.mock._patch object at 0x7648ffad9ac0&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'

/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1428: AttributeError</error></testcase><testcase classname="app.tests.api.integration.test_xgboost_integration.TestXGBoostIntegration" name="test_healthcheck" time="0.000"><error message="failed on setup with &quot;AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'&quot;">@pytest.fixture(autouse=True)
    def mock_auth_dependencies():
        """Mock authentication dependencies for all tests."""
&gt;       with patch("app.api.dependencies.auth.get_current_clinician") as mock_get_clinician, \
             patch("app.api.dependencies.auth.verify_patient_access") as mock_verify_access:

app/tests/api/integration/test_xgboost_integration.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;unittest.mock._patch object at 0x7648ef3a9dc0&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app.api.dependencies.auth' from '/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/api/dependencies/auth.py'&gt; does not have the attribute 'get_current_clinician'

/usr/local/python/3.12.1/lib/python3.12/unittest/mock.py:1428: AttributeError</error></testcase><testcase classname="app.tests.api.routes.test_xgboost_routes" name="test_xgboost_endpoints_return_200[/api/xgboost/risk-prediction-request_model0-RiskPredictionResponse]" time="0.002"><failure message="pydantic_core._pydantic_core.ValidationError: 8 validation errors for RiskPredictionResponse&#10;prediction_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;patient_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;risk_type&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;risk_level&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;risk_score&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;risk_factors&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;timestamp&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;time_frame_days&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing">client = &lt;starlette.testclient.TestClient object at 0x7648ef3fc0e0&gt;
endpoint = '/api/xgboost/risk-prediction'
request_model = RiskPredictionRequest(patient_id='123', risk_type=&lt;RiskType.RELAPSE: 'relapse'&gt;, clinical_data={'risk_factors': ['depression', 'anxiety'], 'previous_episodes': 2, 'current_treatment': 'cbt'}, time_frame_days=30)
response_model = &lt;class 'app.api.schemas.xgboost.RiskPredictionResponse'&gt;

    @pytest.mark.api
    @pytest.mark.parametrize(
        "endpoint, request_model, response_model", [
            (
                "/api/xgboost/risk-prediction",
                RiskPredictionRequest(
                    patient_id="123",
                    risk_type=RiskType.RELAPSE,
                    clinical_data={
                        "risk_factors": ["depression", "anxiety"],
                        "previous_episodes": 2,
                        "current_treatment": "cbt"
                    }
                ),
                RiskPredictionResponse
            ),
            (
                "/api/xgboost/treatment-response",
                TreatmentResponseRequest(
                    patient_id="123",
                    treatment_type=TreatmentType.THERAPY_CBT,
                    treatment_details=TherapyDetails(
                        frequency="weekly",
                        duration_minutes=60,
                        modality="individual"
                    ),
                    clinical_data={
                        "previous_treatments": ["medication_ssri"],
                        "duration_weeks": 12
                    }
                ),
                TreatmentResponseResponse
            ),
            (
                "/api/xgboost/outcome-prediction",
                OutcomePredictionRequest(
                    patient_id="123",
                    outcome_type=OutcomeType.SYMPTOM,
                    outcome_timeframe=TimeFrame(weeks=8),
                    clinical_data={
                        "target_outcome": "remission",
                        "current_symptoms": ["depressed_mood", "insomnia"]
                    },
                    treatment_plan={
                        "medication": "fluoxetine",
                        "therapy": "cbt",
                        "duration_weeks": 12
                    }
                ),
                OutcomePredictionResponse
            ),
            (
                "/api/xgboost/model-info",
                ModelInfoRequest(
                    model_id="xgb-risk-v1",
                    model_type="risk_prediction",
                    include_metrics=True
                ),
                ModelInfoResponse
            ),
        ]
    )
    def test_xgboost_endpoints_return_200(client, endpoint, request_model, response_model):
        """Test that XGBoost endpoints return 200 status code."""
        # Setup mock return value
&gt;       mock_response = response_model(prediction=0.75, confidence=0.85)
E       pydantic_core._pydantic_core.ValidationError: 8 validation errors for RiskPredictionResponse
E       prediction_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       patient_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       risk_type
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       risk_level
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       risk_score
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       risk_factors
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       timestamp
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       time_frame_days
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing

app/tests/api/routes/test_xgboost_routes.py:128: ValidationError</failure></testcase><testcase classname="app.tests.api.routes.test_xgboost_routes" name="test_xgboost_endpoints_return_200[/api/xgboost/treatment-response-request_model1-TreatmentResponseResponse]" time="0.001"><failure message="pydantic_core._pydantic_core.ValidationError: 10 validation errors for TreatmentResponseResponse&#10;prediction_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;patient_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;treatment_type&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;treatment_details&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;response_likelihood&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;efficacy_score&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;expected_outcome&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;side_effect_risk&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;timestamp&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;prediction_horizon&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing">client = &lt;starlette.testclient.TestClient object at 0x7648ef3ffda0&gt;
endpoint = '/api/xgboost/treatment-response'
request_model = TreatmentResponseRequest(patient_id='123', treatment_type=&lt;TreatmentType.THERAPY_CBT: 'therapy_cbt'&gt;, treatment_detail...dual'), clinical_data={'previous_treatments': ['medication_ssri'], 'duration_weeks': 12}, prediction_horizon='8_weeks')
response_model = &lt;class 'app.api.schemas.xgboost.TreatmentResponseResponse'&gt;

    @pytest.mark.api
    @pytest.mark.parametrize(
        "endpoint, request_model, response_model", [
            (
                "/api/xgboost/risk-prediction",
                RiskPredictionRequest(
                    patient_id="123",
                    risk_type=RiskType.RELAPSE,
                    clinical_data={
                        "risk_factors": ["depression", "anxiety"],
                        "previous_episodes": 2,
                        "current_treatment": "cbt"
                    }
                ),
                RiskPredictionResponse
            ),
            (
                "/api/xgboost/treatment-response",
                TreatmentResponseRequest(
                    patient_id="123",
                    treatment_type=TreatmentType.THERAPY_CBT,
                    treatment_details=TherapyDetails(
                        frequency="weekly",
                        duration_minutes=60,
                        modality="individual"
                    ),
                    clinical_data={
                        "previous_treatments": ["medication_ssri"],
                        "duration_weeks": 12
                    }
                ),
                TreatmentResponseResponse
            ),
            (
                "/api/xgboost/outcome-prediction",
                OutcomePredictionRequest(
                    patient_id="123",
                    outcome_type=OutcomeType.SYMPTOM,
                    outcome_timeframe=TimeFrame(weeks=8),
                    clinical_data={
                        "target_outcome": "remission",
                        "current_symptoms": ["depressed_mood", "insomnia"]
                    },
                    treatment_plan={
                        "medication": "fluoxetine",
                        "therapy": "cbt",
                        "duration_weeks": 12
                    }
                ),
                OutcomePredictionResponse
            ),
            (
                "/api/xgboost/model-info",
                ModelInfoRequest(
                    model_id="xgb-risk-v1",
                    model_type="risk_prediction",
                    include_metrics=True
                ),
                ModelInfoResponse
            ),
        ]
    )
    def test_xgboost_endpoints_return_200(client, endpoint, request_model, response_model):
        """Test that XGBoost endpoints return 200 status code."""
        # Setup mock return value
&gt;       mock_response = response_model(prediction=0.75, confidence=0.85)
E       pydantic_core._pydantic_core.ValidationError: 10 validation errors for TreatmentResponseResponse
E       prediction_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       patient_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       treatment_type
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       treatment_details
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       response_likelihood
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       efficacy_score
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       expected_outcome
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       side_effect_risk
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       timestamp
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       prediction_horizon
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing

app/tests/api/routes/test_xgboost_routes.py:128: ValidationError</failure></testcase><testcase classname="app.tests.api.routes.test_xgboost_routes" name="test_xgboost_endpoints_return_200[/api/xgboost/outcome-prediction-request_model2-OutcomePredictionResponse]" time="0.001"><failure message="pydantic_core._pydantic_core.ValidationError: 8 validation errors for OutcomePredictionResponse&#10;prediction_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;patient_id&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;outcome_type&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;outcome_score&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;time_frame_days&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;trajectory&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;outcome_details&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;timestamp&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing">client = &lt;starlette.testclient.TestClient object at 0x7648ee997110&gt;
endpoint = '/api/xgboost/outcome-prediction'
request_model = OutcomePredictionRequest(patient_id='123', outcome_timeframe=TimeFrame(days=None, weeks=8, months=None), clinical_data...an={'medication': 'fluoxetine', 'therapy': 'cbt', 'duration_weeks': 12}, outcome_type=&lt;OutcomeType.SYMPTOM: 'symptom'&gt;)
response_model = &lt;class 'app.api.schemas.xgboost.OutcomePredictionResponse'&gt;

    @pytest.mark.api
    @pytest.mark.parametrize(
        "endpoint, request_model, response_model", [
            (
                "/api/xgboost/risk-prediction",
                RiskPredictionRequest(
                    patient_id="123",
                    risk_type=RiskType.RELAPSE,
                    clinical_data={
                        "risk_factors": ["depression", "anxiety"],
                        "previous_episodes": 2,
                        "current_treatment": "cbt"
                    }
                ),
                RiskPredictionResponse
            ),
            (
                "/api/xgboost/treatment-response",
                TreatmentResponseRequest(
                    patient_id="123",
                    treatment_type=TreatmentType.THERAPY_CBT,
                    treatment_details=TherapyDetails(
                        frequency="weekly",
                        duration_minutes=60,
                        modality="individual"
                    ),
                    clinical_data={
                        "previous_treatments": ["medication_ssri"],
                        "duration_weeks": 12
                    }
                ),
                TreatmentResponseResponse
            ),
            (
                "/api/xgboost/outcome-prediction",
                OutcomePredictionRequest(
                    patient_id="123",
                    outcome_type=OutcomeType.SYMPTOM,
                    outcome_timeframe=TimeFrame(weeks=8),
                    clinical_data={
                        "target_outcome": "remission",
                        "current_symptoms": ["depressed_mood", "insomnia"]
                    },
                    treatment_plan={
                        "medication": "fluoxetine",
                        "therapy": "cbt",
                        "duration_weeks": 12
                    }
                ),
                OutcomePredictionResponse
            ),
            (
                "/api/xgboost/model-info",
                ModelInfoRequest(
                    model_id="xgb-risk-v1",
                    model_type="risk_prediction",
                    include_metrics=True
                ),
                ModelInfoResponse
            ),
        ]
    )
    def test_xgboost_endpoints_return_200(client, endpoint, request_model, response_model):
        """Test that XGBoost endpoints return 200 status code."""
        # Setup mock return value
&gt;       mock_response = response_model(prediction=0.75, confidence=0.85)
E       pydantic_core._pydantic_core.ValidationError: 8 validation errors for OutcomePredictionResponse
E       prediction_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       patient_id
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       outcome_type
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       outcome_score
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       time_frame_days
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       trajectory
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       outcome_details
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       timestamp
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing

app/tests/api/routes/test_xgboost_routes.py:128: ValidationError</failure></testcase><testcase classname="app.tests.api.routes.test_xgboost_routes" name="test_xgboost_endpoints_return_200[/api/xgboost/model-info-request_model3-ModelInfoResponse]" time="0.001"><failure message="pydantic_core._pydantic_core.ValidationError: 8 validation errors for ModelInfoResponse&#10;model_type&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;version&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;last_updated&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;description&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;features&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;performance_metrics&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;hyperparameters&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing&#10;status&#10;  Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]&#10;    For further information visit https://errors.pydantic.dev/2.9/v/missing">client = &lt;starlette.testclient.TestClient object at 0x7648ee97e4e0&gt;
endpoint = '/api/xgboost/model-info'
request_model = ModelInfoRequest(model_type='risk_prediction')
response_model = &lt;class 'app.api.schemas.xgboost.ModelInfoResponse'&gt;

    @pytest.mark.api
    @pytest.mark.parametrize(
        "endpoint, request_model, response_model", [
            (
                "/api/xgboost/risk-prediction",
                RiskPredictionRequest(
                    patient_id="123",
                    risk_type=RiskType.RELAPSE,
                    clinical_data={
                        "risk_factors": ["depression", "anxiety"],
                        "previous_episodes": 2,
                        "current_treatment": "cbt"
                    }
                ),
                RiskPredictionResponse
            ),
            (
                "/api/xgboost/treatment-response",
                TreatmentResponseRequest(
                    patient_id="123",
                    treatment_type=TreatmentType.THERAPY_CBT,
                    treatment_details=TherapyDetails(
                        frequency="weekly",
                        duration_minutes=60,
                        modality="individual"
                    ),
                    clinical_data={
                        "previous_treatments": ["medication_ssri"],
                        "duration_weeks": 12
                    }
                ),
                TreatmentResponseResponse
            ),
            (
                "/api/xgboost/outcome-prediction",
                OutcomePredictionRequest(
                    patient_id="123",
                    outcome_type=OutcomeType.SYMPTOM,
                    outcome_timeframe=TimeFrame(weeks=8),
                    clinical_data={
                        "target_outcome": "remission",
                        "current_symptoms": ["depressed_mood", "insomnia"]
                    },
                    treatment_plan={
                        "medication": "fluoxetine",
                        "therapy": "cbt",
                        "duration_weeks": 12
                    }
                ),
                OutcomePredictionResponse
            ),
            (
                "/api/xgboost/model-info",
                ModelInfoRequest(
                    model_id="xgb-risk-v1",
                    model_type="risk_prediction",
                    include_metrics=True
                ),
                ModelInfoResponse
            ),
        ]
    )
    def test_xgboost_endpoints_return_200(client, endpoint, request_model, response_model):
        """Test that XGBoost endpoints return 200 status code."""
        # Setup mock return value
&gt;       mock_response = response_model(prediction=0.75, confidence=0.85)
E       pydantic_core._pydantic_core.ValidationError: 8 validation errors for ModelInfoResponse
E       model_type
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       version
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       last_updated
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       description
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       features
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       performance_metrics
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       hyperparameters
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing
E       status
E         Field required [type=missing, input_value={'prediction': 0.75, 'confidence': 0.85}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.9/v/missing

app/tests/api/routes/test_xgboost_routes.py:128: ValidationError</failure></testcase><testcase classname="app.tests.api.routes.test_xgboost_routes" name="test_xgboost_risk_prediction_with_invalid_data" time="0.028"><failure message="RecursionError: maximum recursion depth exceeded">client = &lt;starlette.testclient.TestClient object at 0x7648ee9bd8e0&gt;

    @pytest.mark.api
    def test_xgboost_risk_prediction_with_invalid_data(client):
        """Test that risk prediction endpoint validates input data."""
        # Invalid request missing required fields
&gt;       response = client.post(
            "/api/xgboost/risk-prediction",
            json={}
        )

app/tests/api/routes/test_xgboost_routes.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/testclient.py:538: in post
    return super().post(
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:1132: in post
    return self.request(
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/testclient.py:437: in request
    return super().request(
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:814: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:901: in send
    response = self._send_handling_auth(
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:929: in _send_handling_auth
    response = self._send_handling_redirects(
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:966: in _send_handling_redirects
    response = self._send_single_request(request)
/usr/local/python/3.12.1/lib/python3.12/site-packages/httpx/_client.py:1002: in _send_single_request
    response = transport.handle_request(request)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/testclient.py:340: in handle_request
    raise exc
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/testclient.py:337: in handle_request
    portal.call(self.app, scope, receive, send)
/home/codespace/.local/lib/python3.12/site-packages/anyio/from_thread.py:290: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
/home/codespace/.local/lib/python3.12/site-packages/anyio/from_thread.py:221: in _call_func
    retval = await retval_or_awaitable
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/applications.py:1054: in __call__
    await super().__call__(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/applications.py:112: in __call__
    await self.middleware_stack(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/middleware/errors.py:187: in __call__
    raise exc
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/middleware/errors.py:165: in __call__
    await self.app(scope, receive, _send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/middleware/exceptions.py:62: in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/_exception_handler.py:53: in wrapped_app
    raise exc
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py:714: in __call__
    await self.middleware_stack(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py:734: in app
    await route.handle(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py:288: in handle
    await self.app(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py:76: in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/_exception_handler.py:53: in wrapped_app
    raise exc
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
/usr/local/python/3.12.1/lib/python3.12/site-packages/starlette/routing.py:73: in app
    response = await f(request)
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py:327: in app
    content = await serialize_response(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/routing.py:201: in serialize_response
    return jsonable_encoder(response_content)
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:333: in jsonable_encoder
    return jsonable_encoder(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:289: in jsonable_encoder
    encoded_value = jsonable_encoder(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:333: in jsonable_encoder
    return jsonable_encoder(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:289: in jsonable_encoder
    encoded_value = jsonable_encoder(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:333: in jsonable_encoder
    return jsonable_encoder(
/usr/local/python/3.12.1/lib/python3.12/site-packages/fastapi/encoders.py:289: in jsonable_encoder
    encoded_value = jsonable_encoder(
E   RecursionError: maximum recursion depth exceeded
!!! Recursion detected (same locals &amp; position)</failure></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_predict_risk_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 202&#10;  def test_predict_risk_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:202&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 202
  def test_predict_risk_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:202</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_predict_treatment_response_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 244&#10;  def test_predict_treatment_response_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:244&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 244
  def test_predict_treatment_response_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:244</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_predict_outcome_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 287&#10;  def test_predict_outcome_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:287&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 287
  def test_predict_outcome_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:287</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_get_feature_importance_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 333&#10;  def test_get_feature_importance_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:333&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 333
  def test_get_feature_importance_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:333</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_digital_twin_integration_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 359&#10;  def test_digital_twin_integration_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:359&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 359
  def test_digital_twin_integration_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:359</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_model_info_endpoint" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 384&#10;  def test_model_info_endpoint(client: TestClient, mock_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:384&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 384
  def test_model_info_endpoint(client: TestClient, mock_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:384</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_model_not_found_error" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 410&#10;  def test_model_not_found_error(client: TestClient, mock_error_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:410&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 410
  def test_model_not_found_error(client: TestClient, mock_error_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:410</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_prediction_error" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 431&#10;  def test_prediction_error(client: TestClient, mock_error_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:431&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 431
  def test_prediction_error(client: TestClient, mock_error_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:431</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_service_connection_error" time="0.000"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 453&#10;  def test_service_connection_error(client: TestClient, mock_error_dependencies):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:453&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 453
  def test_service_connection_error(client: TestClient, mock_error_dependencies):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:453</error></testcase><testcase classname="app.tests.api.unit.test_xgboost_endpoints" name="test_validation_error" time="0.001"><error message="failed on setup with &quot;file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 477&#10;  def test_validation_error(client: TestClient):&#10;E       fixture 'client' not found&#10;&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:477&quot;">file /workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py, line 477
  def test_validation_error(client: TestClient):
E       fixture 'client' not found
&gt;       available fixtures: _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, extra, extras, faker, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, include_metadata_in_junit_xml, metadata, mock_dependencies, mock_error_dependencies, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_environment, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

/workspaces/Novamind-Backend-ONLY-TWINS/backend/app/tests/api/unit/test_xgboost_endpoints.py:477</error></testcase></testsuite></testsuites>