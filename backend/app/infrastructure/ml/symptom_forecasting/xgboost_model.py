# -*- coding: utf-8 -*-
"""
XGBoost-based symptom forecasting model for the NOVAMIND Digital Twin.

This module implements an XGBoost model with Bayesian Hyperparameter Optimization
for psychiatric symptom forecasting, following the architecture described in
the AI Models Core Implementation documentation.
"""

import logging
import os
from datetime import datetime
from app.domain.utils.datetime_utils import UTC
from typing import Any, Dict, List, Optional, Tuple

import joblib
import numpy as np
import optuna
import xgboost as xgb
from optuna.samplers import TPESampler
from app.core.services.ml.xgboost.exceptions import PredictionError, ValidationError


class XGBoostSymptomModel:
    """
    XGBoost model for psychiatric symptom forecasting with Bayesian optimization.

    This class implements the XGBoost component of the symptom forecasting ensemble,
    providing interpretable predictions with feature importance.
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        feature_names: Optional[List[str]] = None,
        target_names: Optional[List[str]] = None,
        n_estimators: int = 1000,
        learning_rate: float = 0.01,
        max_depth: int = 6,
        subsample: float = 0.8,
        colsample_bytree: float = 0.8,
        objective: str = "reg:squarederror",
        random_state: int = 42,
    ):
        """
        Initialize the XGBoost symptom model.

        Args:
            model_path: Optional path to a pretrained model
            feature_names: Names of input features
            target_names: Names of target variables
            n_estimators: Number of boosting rounds
            learning_rate: Learning rate
            max_depth: Maximum tree depth
            subsample: Subsample ratio of the training instances
            colsample_bytree: Subsample ratio of columns when constructing each tree
            objective: Objective function
            random_state: Random seed
        """
        self.feature_names = feature_names
        self.target_names = target_names
        self.models = {}  # Dictionary to store one model per target variable

        # Default parameters
        self.params = {
            "n_estimators": n_estimators,
            "learning_rate": learning_rate,
            "max_depth": max_depth,
            "subsample": subsample,
            "colsample_bytree": colsample_bytree,
            "objective": objective,
            "random_state": random_state,
            "tree_method": "hist",  # For faster training
            "verbosity": 0,
        }

        # Load pretrained model if provided
        if model_path and os.path.exists(model_path):
            self._load_model(model_path)

    def _load_model(self, model_path: str) -> None:
        """
        Load a pretrained model from disk.

        Args:
            model_path: Path to the model file
        """
        try:
            model_data = joblib.load(model_path)
            self.models = model_data.get("models", {})
            self.feature_names = model_data.get("feature_names")
            self.target_names = model_data.get("target_names")
            self.params = model_data.get("params", self.params)
            logging.info(f"Loaded XGBoost model from {model_path}")
        except Exception as e:
            logging.error(f"Error loading XGBoost model: {str(e)}")
            raise Exception(f"Failed to load XGBoost model: {str(e)}")

    def save_model(self, model_path: str) -> None:
        """
        Save the trained model to disk.

        Args:
            model_path: Path to save the model
        """
        model_data = {
            "models": self.models,
            "feature_names": self.feature_names,
            "target_names": self.target_names,
            "params": self.params,
            "timestamp": datetime.now(UTC).isoformat(),
        }

        try:
            joblib.dump(model_data, model_path)
            logging.info(f"Saved XGBoost model to {model_path}")
        except Exception as e:
            logging.error(f"Error saving XGBoost model: {str(e)}")
            raise Exception(f"Failed to save XGBoost model: {str(e)}")

    def _optimize_hyperparameters(
        self, X_train: np.ndarray, y_train: np.ndarray, n_trials: int = 50
    ) -> Dict[str, Any]:
        """
        Optimize hyperparameters using Optuna.

        Args:
            X_train: Training features
            y_train: Training targets
            n_trials: Number of optimization trials

        Returns:
            Dictionary of optimized hyperparameters
        """

        def objective(trial):
            params = {
                "learning_rate": trial.suggest_float(
                    "learning_rate", 0.001, 0.1, log=True
                ),
                "max_depth": trial.suggest_int("max_depth", 3, 10),
                "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
                "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
                "gamma": trial.suggest_float("gamma", 0, 1),
                "alpha": trial.suggest_float("alpha", 0, 1),
                "lambda": trial.suggest_float("lambda", 0, 1),
                "objective": "reg:squarederror",
                "tree_method": "hist",
                "verbosity": 0,
            }

            # Use 5-fold cross-validation
            dtrain = xgb.DMatrix(X_train, label=y_train)
            cv_results = xgb.cv(
                params,
                dtrain,
                num_boost_round=1000,
                early_stopping_rounds=50,
                nfold=5,
                metrics="rmse",
                seed=42,
            )

            # Return the best RMSE
            return cv_results["test-rmse-mean"].min()

        # Create Optuna study
        sampler = TPESampler(seed=42)
        study = optuna.create_study(direction="minimize", sampler=sampler)
        study.optimize(objective, n_trials=n_trials)

        # Get best parameters
        best_params = study.best_params
        best_params["n_estimators"] = int(
            study.best_trial.user_attrs.get("n_estimators", 1000)
        )
        best_params["objective"] = "reg:squarederror"
        best_params["tree_method"] = "hist"
        best_params["verbosity"] = 0

        logging.info(f"Optimized hyperparameters: {best_params}")
        return best_params

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        optimize: bool = True,
        n_trials: int = 50,
    ) -> Dict[str, Any]:
        """
        Train the XGBoost model.

        Args:
            X_train: Training features
            y_train: Training targets
            optimize: Whether to optimize hyperparameters
            n_trials: Number of optimization trials

        Returns:
            Dictionary containing training results
        """
        if X_train.shape[0] != y_train.shape[0]:
            raise Exception(
                f"X_train and y_train must have the same number of samples, got {X_train.shape[0]} and {y_train.shape[0]}"
            )

        # Set feature names if not provided
        if self.feature_names is None:
            self.feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]

        # Set target names if not provided
        if self.target_names is None:
            if y_train.ndim == 1:
                self.target_names = ["target"]
            else:
                self.target_names = [f"target_{i}" for i in range(y_train.shape[1])]

        # Optimize hyperparameters if requested
        if optimize:
            self.params = self._optimize_hyperparameters(X_train, y_train, n_trials)

        # Train one model for each target variable
        training_results = {}

        if y_train.ndim == 1:
            # Single target variable
            dtrain = xgb.DMatrix(
                X_train, label=y_train, feature_names=self.feature_names
            )
            model = xgb.train(
                self.params, dtrain, num_boost_round=self.params["n_estimators"]
            )
            self.models[self.target_names[0]] = model

            # Calculate feature importance
            importance = model.get_score(importance_type="gain")
            training_results[self.target_names[0]] = {
                "feature_importance": importance,
                "params": self.params,
            }
        else:
            # Multiple target variables
            for i, target_name in enumerate(self.target_names):
                dtrain = xgb.DMatrix(
                    X_train, label=y_train[:, i], feature_names=self.feature_names
                )
                model = xgb.train(
                    self.params, dtrain, num_boost_round=self.params["n_estimators"]
                )
                self.models[target_name] = model

                # Calculate feature importance
                importance = model.get_score(importance_type="gain")
                training_results[target_name] = {
                    "feature_importance": importance,
                    "params": self.params,
                }

        return {
            "training_results": training_results,
            "feature_names": self.feature_names,
            "target_names": self.target_names,
            "params": self.params,
        }

    async def predict(self, X: np.ndarray, horizon: int) -> Dict[str, Any]:
        """
        Generate predictions for the given input data.

        Args:
            X: Input features
            horizon: Forecast horizon

        Returns:
            Dictionary containing prediction results
        """
        if not self.models:
            raise Exception("Model has not been trained or loaded")

        try:
            # Check if feature names match
            if self.feature_names and X.shape[1] != len(self.feature_names):
                raise Exception(
                    f"Input has {X.shape[1]} features, but model expects {len(self.feature_names)}"
                )

            # Prepare DMatrix
            dmatrix = xgb.DMatrix(X, feature_names=self.feature_names)

            # Make predictions for each target
            predictions = {}
            feature_importance = {}

            for target_name, model in self.models.items():
                # Get predictions
                pred = model.predict(dmatrix)
                predictions[target_name] = pred

                # Get feature importance
                importance = model.get_score(importance_type="gain")
                feature_importance[target_name] = importance

            # For multi-step forecasting, we need to generate predictions for each step
            # This is a simplified implementation - in practice, we would use a more sophisticated approach
            multi_step_predictions = {}

            for target_name in self.target_names:
                # Initialize array for multi-step predictions
                multi_step = np.zeros((X.shape[0], horizon))

                # Use the last prediction as input for the next step (autoregressive)
                current_input = X.copy()

                for h in range(horizon):
                    # Make prediction for current step
                    dmatrix = xgb.DMatrix(
                        current_input, feature_names=self.feature_names
                    )
                    step_pred = self.models[target_name].predict(dmatrix)

                    # Store prediction
                    multi_step[:, h] = step_pred

                    # Update input for next step (simplified approach)
                    # In practice, this would depend on the specific feature engineering
                    current_input = np.roll(current_input, -1, axis=1)
                    current_input[:, -1] = step_pred

                multi_step_predictions[target_name] = multi_step

            # Combine all predictions
            combined_predictions = np.zeros(
                (X.shape[0], horizon, len(self.target_names))
            )

            for i, target_name in enumerate(self.target_names):
                combined_predictions[:, :, i] = multi_step_predictions[target_name]

            return {
                "values": combined_predictions,
                "feature_importance": feature_importance,
                "model_type": "xgboost",
            }

        except Exception as e:
            raise PredictionError(f"Error during XGBoost model inference: {str(e)}")

    def get_feature_importance(self) -> Dict[str, Dict[str, float]]:
        """
        Get feature importance for each target variable.

        Returns:
            Dictionary mapping target names to feature importance dictionaries
        """
        if not self.models:
            raise Exception("Model has not been trained or loaded")

        feature_importance = {}

        for target_name, model in self.models.items():
            importance = model.get_score(importance_type="gain")
            feature_importance[target_name] = importance

        return feature_importance

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the model.

        Returns:
            Dictionary containing model information
        """
        return {
            "model_type": "XGBoost with Bayesian Hyperparameter Optimization",
            "feature_names": self.feature_names,
            "target_names": self.target_names,
            "params": self.params,
            "num_models": len(self.models),
            "timestamp": datetime.now(UTC).isoformat(),
        }
